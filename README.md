# Teste_Software_LLM_2025_Santos_Augusto_Silva_David_Lima_Giulian_Lima_Yasmim

# ğŸ“„ Tutorial â€” Uso de LLMs para GeraÃ§Ã£o de Testes Baseados em Propriedades (PBT)

Este tutorial foi elaborado para atender aos requisitos da atividade proposta, explorando o uso de **Large Language Models (LLMs)** na geraÃ§Ã£o de **Property-Based Tests (PBT)**, com base no artigo:

> **[Can Large Language Models Write Good Property-Based Tests?](https://arxiv.org/abs/2307.04346)**

O documento cobre desde a escolha do artigo atÃ© a sugestÃ£o de melhorias e identificaÃ§Ã£o de um problema relevante no Stack Overflow.

---

## ğŸ“Œ a) Qual artigo escolhemos?

- **TÃ­tulo:** Can Large Language Models Write Good Property-Based Tests?  
- **SeÃ§Ã£o:** Unit Test Case Generation  
- **Link:** [arxiv:2307.04346](https://arxiv.org/abs/2307.04346)  

---

## ğŸ¯ b) Qual o problema que esse artigo tenta resolver?

Escrever bons testes de software Ã© essencial, mas trabalhoso.  
O artigo aborda um tipo especÃ­fico: **Property-Based Testing (PBT)**.  

Ao contrÃ¡rio dos testes tradicionais, que validam entradas especÃ­ficas, o PBT verifica **propriedades invariantes** de uma funÃ§Ã£o em mÃºltiplas entradas.  
O problema Ã© que criar esses testes exige **tempo, conhecimento tÃ©cnico e criatividade**.

---

## ğŸ’¡ c) Qual a soluÃ§Ã£o proposta?

O artigo apresenta a **PBT-GPT**, uma ferramenta baseada em LLMs (como GPT-4) que gera automaticamente testes PBT a partir da **descriÃ§Ã£o de funÃ§Ãµes**.  
O modelo detecta propriedades como **simetria, comutatividade, idempotÃªncia**, entre outras.

ğŸ“Œ **Objetivo:** automatizar a criaÃ§Ã£o de testes inteligentes sem exigir que o desenvolvedor escreva manualmente cada caso.

---

## ğŸ§  d) Modelos utilizados

- **GPT-4** â€” [OpenAI](https://platform.openai.com)  
- **Claude 2.1** â€” [Anthropic](https://www.anthropic.com)  
- **Gemini 1.5 Pro** â€” [Google](https://ai.google.dev/gemini/)  

Todos sÃ£o modelos **decoder-only**, baseados na arquitetura **Transformers**.

---

## ğŸ—‚ e) Dataset e cÃ³digo

- **Dataset:** 40 funÃ§Ãµes populares de bibliotecas Python  
  - NumPy  
  - datetime  
  - networkx  

- **CÃ³digo:** [GitHub - PBT-GPT](https://github.com/engineeringforresearch/PBT-GPT)  

---

## ğŸ§ª f) Como testaram a soluÃ§Ã£o?

CritÃ©rios de avaliaÃ§Ã£o:

1. **Validez:** o teste executa sem erros?  
2. **Soundness:** o teste detecta erros reais?  
3. **Cobertura:** o teste verifica propriedades realmente Ãºteis?  

ğŸ“Š **Resultado:** GPT-4 apresentou o melhor desempenho, com **>60% de testes vÃ¡lidos** e boa cobertura.

---

## ğŸ“ g) Exemplo prÃ¡tico

FunÃ§Ã£o: `sorted(lst)`  
Teste gerado usando **Hypothesis**:

```python
from hypothesis import given
import hypothesis.strategies as st

@given(st.lists(st.integers()))
def test_sorted_is_idempotent(lst):
    assert sorted(sorted(lst)) == sorted(lst)
```
ğŸ“Œ Propriedade testada: IdempotÃªncia â€” aplicar sorted mais de uma vez retorna o mesmo resultado que aplicar apenas uma vez.

âš™ï¸ h) Como isso pode ajudar no dia a dia?
Garantia de qualidade para novas bibliotecas e funÃ§Ãµes

DetecÃ§Ã£o de bugs antes de chegarem ao usuÃ¡rio

ReduÃ§Ã£o de tempo na criaÃ§Ã£o de testes

Ideal para equipes com pouco conhecimento prÃ©vio de PBT

AplicÃ¡vel em pipelines CI/CD

âš ï¸ i) LimitaÃ§Ãµes
Nem todos os testes gerados sÃ£o vÃ¡lidos

Possibilidade de repetiÃ§Ã£o ou pouca utilidade nos testes

Alto custo de uso contÃ­nuo de modelos como GPT-4

Qualidade depende fortemente do prompt

Testes nem sempre sÃ£o fÃ¡ceis de entender

ğŸš€ j) PossÃ­veis melhorias
Fine-tuning com mais exemplos reais de testes

Uso de few-shot prompting para fornecer exemplos ao modelo

IntegraÃ§Ã£o com mutation testing para validar automaticamente a utilidade dos testes gerados

ğŸ’¬ k) Problema do Stack Overflow relacionado
TÃ­tulo: How can I generate property-based tests for a Python function using Hypothesis?

Motivo da escolha: o problema Ã© exatamente o que o artigo resolve â€” gerar testes PBT de forma automÃ¡tica a partir de uma descriÃ§Ã£o de funÃ§Ã£o.

ğŸ“š ReferÃªncia
Spruit, N., Wesselink, W., & others. (2023). Can Large Language Models Write Good Property-Based Tests? arXiv:2307.04346.
DisponÃ­vel em: https://arxiv.org/abs/2307.04346
